<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Integrating CrawlAI with Directus and Appsmith - Tutorial</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 1.5em;
        }
        h1 {
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            border-bottom: 1px solid #bdc3c7;
            padding-bottom: 5px;
        }
        code {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 3px;
            font-family: Consolas, Monaco, 'Andale Mono', monospace;
            padding: 2px 4px;
            font-size: 0.9em;
        }
        pre {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 3px;
            padding: 10px;
            overflow: auto;
            font-family: Consolas, Monaco, 'Andale Mono', monospace;
            line-height: 1.4;
        }
        .step {
            background-color: #f9f9f9;
            border-left: 4px solid #27ae60;
            padding: 15px;
            margin: 20px 0;
        }
        .step-number {
            font-size: 1.2em;
            font-weight: bold;
            color: #27ae60;
            margin-bottom: 10px;
        }
        img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 5px;
            margin: 10px 0;
        }
        .note {
            background-color: #fff8dc;
            border-left: 4px solid #f1c40f;
            padding: 10px 15px;
            margin: 20px 0;
        }
        .warning {
            background-color: #ffebee;
            border-left: 4px solid #e74c3c;
            padding: 10px 15px;
            margin: 20px 0;
        }
        .nav {
            background-color: #f8f9fa;
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .nav a {
            margin-right: 15px;
            text-decoration: none;
            color: #3498db;
        }
        .nav a:hover {
            text-decoration: underline;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        .code-block {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 3px;
            padding: 15px;
            margin: 15px 0;
            overflow-x: auto;
        }
        .screenshot {
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 5px;
            margin: 15px 0;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .flex-container {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }
        .flex-item {
            flex: 1;
            min-width: 300px;
        }
        .architecture-diagram {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>Integrating CrawlAI with Directus and Appsmith - Tutorial</h1>
    
    <div class="nav">
        <a href="#overview">Overview</a>
        <a href="#architecture">Architecture</a>
        <a href="#prerequisites">Prerequisites</a>
        <a href="#setup-directus">Setting Up Directus</a>
        <a href="#setup-crawlai">Configuring CrawlAI API</a>
        <a href="#setup-appsmith">Setting Up Appsmith</a>
        <a href="#integration">Integration Steps</a>
        <a href="#building-ui">Building the UI</a>
        <a href="#advanced">Advanced Features</a>
    </div>
    
    <h2 id="overview">Overview</h2>
    <p>
        This tutorial will guide you through creating a complete web crawling solution by integrating three powerful technologies:
    </p>
    <ul>
        <li><strong>CrawlAI API</strong>: A powerful web crawling service that extracts content from websites</li>
        <li><strong>Directus</strong>: A headless CMS that will store and manage our crawled data</li>
        <li><strong>Appsmith</strong>: A low-code platform for building custom admin panels and dashboards</li>
    </ul>
    
    <p>
        By the end of this tutorial, you'll have a complete system that allows you to:
    </p>
    <ul>
        <li>Configure and trigger web crawls through a user-friendly interface</li>
        <li>Store crawled data in a structured database</li>
        <li>View, search, and analyze crawled content</li>
        <li>Visualize crawl statistics and insights</li>
        <li>Manage user permissions and access controls</li>
    </ul>
    
    <h2 id="architecture">Architecture Overview</h2>
    <div class="architecture-diagram">
        <pre>
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│             │     │             │     │             │
│  Appsmith   │◄────┤  Directus   │◄────┤  CrawlAI    │
│  (Frontend) │     │  (Storage)  │     │  (Crawler)  │
│             │     │             │     │             │
└──────┬──────┘     └──────┬──────┘     └──────┬──────┘
       │                   │                   │
       ▼                   ▼                   ▼
┌─────────────────────────────────────────────────────┐
│                                                     │
│                    User Interface                   │
│                                                     │
└─────────────────────────────────────────────────────┘
        </pre>
        <p><em>Figure 1: System Architecture</em></p>
    </div>
    
    <p>
        In this architecture:
    </p>
    <ol>
        <li>Users interact with the Appsmith frontend to configure and trigger crawls</li>
        <li>Appsmith communicates with Directus to store crawl configurations and retrieve results</li>
        <li>Directus triggers the CrawlAI API to perform web crawls</li>
        <li>CrawlAI sends crawl results back to Directus for storage</li>
        <li>Appsmith displays the crawled data and analytics to users</li>
    </ol>
    
    <h2 id="prerequisites">Prerequisites</h2>
    <div class="step">
        <div class="step-number">Before You Begin</div>
        <p>Make sure you have the following:</p>
        <ul>
            <li>A running instance of the CrawlAI API server</li>
            <li>Docker installed (for running Directus and Appsmith)</li>
            <li>Basic knowledge of REST APIs and JavaScript</li>
            <li>A code editor of your choice</li>
        </ul>
        
        <div class="note">
            <strong>Note:</strong> This tutorial assumes your CrawlAI API is running at <code>http://your-server:5000</code>. 
            Replace this with your actual API endpoint throughout the tutorial.
        </div>
    </div>
    
    <h2 id="setup-directus">Setting Up Directus</h2>
    <div class="step">
        <div class="step-number">Step 1: Install and Configure Directus</div>
        <p>Let's start by setting up Directus using Docker:</p>
        
        <ol>
            <li>Create a new directory for your project:</li>
        </ol>
        <div class="code-block">
            <code>mkdir crawlai-integration</code><br>
            <code>cd crawlai-integration</code>
        </div>
        
        <ol start="2">
            <li>Create a <code>docker-compose.yml</code> file with the following content:</li>
        </ol>
        <pre>
version: '3'
services:
  directus:
    image: directus/directus:latest
    ports:
      - 8055:8055
    environment:
      KEY: 'replace-with-random-key'
      SECRET: 'replace-with-random-secret'
      ADMIN_EMAIL: 'admin@example.com'
      ADMIN_PASSWORD: 'directus-password'
      DB_CLIENT: 'sqlite3'
      DB_FILENAME: '/directus/database/data.db'
    volumes:
      - ./directus/database:/directus/database
      - ./directus/uploads:/directus/uploads
    restart: unless-stopped</pre>
        
        <ol start="3">
            <li>Start Directus using Docker Compose:</li>
        </ol>
        <div class="code-block">
            <code>docker-compose up -d directus</code>
        </div>
        
        <ol start="4">
            <li>Access Directus at <a href="http://localhost:8055" target="_blank">http://localhost:8055</a> and log in with:</li>
        </ol>
        <ul>
            <li>Email: <code>admin@example.com</code></li>
            <li>Password: <code>directus-password</code></li>
        </ul>
    </div>
    
    <div class="step">
        <div class="step-number">Step 2: Create Directus Collections</div>
        <p>Now, let's create the necessary collections in Directus to store our crawl data:</p>
        
        <h3>Creating the Crawl Configurations Collection</h3>
        <ol>
            <li>In Directus, go to <strong>Settings</strong> > <strong>Data Model</strong></li>
            <li>Click <strong>Create Collection</strong></li>
            <li>Enter "crawl_configurations" as the collection name</li>
            <li>Add the following fields:</li>
        </ol>
        
        <table>
            <tr>
                <th>Field Name</th>
                <th>Type</th>
                <th>Required</th>
                <th>Notes</th>
            </tr>
            <tr>
                <td>id</td>
                <td>UUID</td>
                <td>Yes</td>
                <td>Primary key, auto-generated</td>
            </tr>
            <tr>
                <td>name</td>
                <td>String</td>
                <td>Yes</td>
                <td>Name of the crawl configuration</td>
            </tr>
            <tr>
                <td>url</td>
                <td>String</td>
                <td>Yes</td>
                <td>Target URL to crawl</td>
            </tr>
            <tr>
                <td>depth</td>
                <td>Integer</td>
                <td>Yes</td>
                <td>Default: 1</td>
            </tr>
            <tr>
                <td>max_pages</td>
                <td>Integer</td>
                <td>No</td>
                <td>Default: 100</td>
            </tr>
            <tr>
                <td>extract_images</td>
                <td>Boolean</td>
                <td>No</td>
                <td>Default: false</td>
            </tr>
            <tr>
                <td>follow_external_links</td>
                <td>Boolean</td>
                <td>No</td>
                <td>Default: false</td>
            </tr>
            <tr>
                <td>excluded_domains</td>
                <td>JSON</td>
                <td>No</td>
                <td>Array of domains to exclude</td>
            </tr>
            <tr>
                <td>schedule</td>
                <td>String</td>
                <td>No</td>
                <td>Cron expression for scheduling</td>
            </tr>
            <tr>
                <td>user_created</td>
                <td>User</td>
                <td>Yes</td>
                <td>User who created the configuration</td>
            </tr>
            <tr>
                <td>created_at</td>
                <td>Timestamp</td>
                <td>Yes</td>
                <td>Auto-generated</td>
            </tr>
            <tr>
                <td>updated_at</td>
                <td>Timestamp</td>
                <td>Yes</td>
                <td>Auto-generated</td>
            </tr>
        </table>
        
        <h3>Creating the Crawl Jobs Collection</h3>
        <ol>
            <li>Go to <strong>Settings</strong> > <strong>Data Model</strong></li>
            <li>Click <strong>Create Collection</strong></li>
            <li>Enter "crawl_jobs" as the collection name</li>
            <li>Add the following fields:</li>
        </ol>
        
        <table>
            <tr>
                <th>Field Name</th>
                <th>Type</th>
                <th>Required</th>
                <th>Notes</th>
            </tr>
            <tr>
                <td>id</td>
                <td>UUID</td>
                <td>Yes</td>
                <td>Primary key, auto-generated</td>
            </tr>
            <tr>
                <td>configuration</td>
                <td>M2O</td>
                <td>Yes</td>
                <td>Relation to crawl_configurations</td>
            </tr>
            <tr>
                <td>task_id</td>
                <td>String</td>
                <td>No</td>
                <td>ID returned by the CrawlAI API</td>
            </tr>
            <tr>
                <td>status</td>
                <td>String</td>
                <td>Yes</td>
                <td>Options: pending, running, completed, failed</td>
            </tr>
            <tr>
                <td>started_at</td>
                <td>Timestamp</td>
                <td>No</td>
                <td>When the job started</td>
            </tr>
            <tr>
                <td>completed_at</td>
                <td>Timestamp</td>
                <td>No</td>
                <td>When the job completed</td>
            </tr>
            <tr>
                <td>error_message</td>
                <td>Text</td>
                <td>No</td>
                <td>Error message if job failed</td>
            </tr>
            <tr>
                <td>user_created</td>
                <td>User</td>
                <td>Yes</td>
                <td>User who triggered the job</td>
            </tr>
            <tr>
                <td>created_at</td>
                <td>Timestamp</td>
                <td>Yes</td>
                <td>Auto-generated</td>
            </tr>
        </table>
        
        <h3>Creating the Crawl Results Collection</h3>
        <ol>
            <li>Go to <strong>Settings</strong> > <strong>Data Model</strong></li>
            <li>Click <strong>Create Collection</strong></li>
            <li>Enter "crawl_results" as the collection name</li>
            <li>Add the following fields:</li>
        </ol>
        
        <table>
            <tr>
                <th>Field Name</th>
                <th>Type</th>
                <th>Required</th>
                <th>Notes</th>
            </tr>
            <tr>
                <td>id</td>
                <td>UUID</td>
                <td>Yes</td>
                <td>Primary key, auto-generated</td>
            </tr>
            <tr>
                <td>job</td>
                <td>M2O</td>
                <td>Yes</td>
                <td>Relation to crawl_jobs</td>
            </tr>
            <tr>
                <td>url</td>
                <td>String</td>
                <td>Yes</td>
                <td>URL that was crawled</td>
            </tr>
            <tr>
                <td>content</td>
                <td>Text</td>
                <td>No</td>
                <td>Extracted content (markdown)</td>
            </tr>
            <tr>
                <td>links</td>
                <td>JSON</td>
                <td>No</td>
                <td>Links found during crawl</td>
            </tr>
            <tr>
                <td>images</td>
                <td>JSON</td>
                <td>No</td>
                <td>Images found during crawl</td>
            </tr>
            <tr>
                <td>metadata</td>
                <td>JSON</td>
                <td>No</td>
                <td>Additional metadata</td>
            </tr>
            <tr>
                <td>pages_crawled</td>
                <td>Integer</td>
                <td>No</td>
                <td>Number of pages crawled</td>
            </tr>
            <tr>
                <td>crawl_time</td>
                <td>Float</td>
                <td>No</td>
                <td>Time taken to complete the crawl (seconds)</td>
            </tr>
            <tr>
                <td>created_at</td>
                <td>Timestamp</td>
                <td>Yes</td>
                <td>Auto-generated</td>
            </tr>
        </table>
        
        <h3>Setting Up Relations</h3>
        <p>Now let's set up the relations between our collections:</p>
        <ol>
            <li>In the crawl_jobs collection, edit the "configuration" field</li>
            <li>Set the Related Collection to "crawl_configurations"</li>
            <li>Set the Related Field to "id"</li>
        </ol>
        
        <ol>
            <li>In the crawl_results collection, edit the "job" field</li>
            <li>Set the Related Collection to "crawl_jobs"</li>
            <li>Set the Related Field to "id"</li>
        </ol>
    </div>
    
    <h2 id="setup-crawlai">Configuring CrawlAI API</h2>
    <div class="step">
        <div class="step-number">Step 3: Set Up CrawlAI API Integration</div>
        
        <h3>Creating a Webhook Endpoint in Directus</h3>
        <p>We need to create a webhook endpoint in Directus to receive callbacks from the CrawlAI API:</p>
        <ol>
            <li>Go to <strong>Settings</strong> > <strong>Project Settings</strong></li>
            <li>Scroll down to the "Custom Endpoints" section</li>
            <li>Click <strong>Add Endpoint</strong></li>
            <li>Configure the endpoint:
                <ul>
                    <li>Name: "crawl-callback"</li>
                    <li>Method: "POST"</li>
                    <li>Path: "/webhook/crawl-callback"</li>
                </ul>
            </li>
            <li>Enter the following code:</li>
        </ol>
        
        <pre>
module.exports = async (req, res) => {
  const { services, exceptions } = req.accountability;
  const { ItemsService } = services;
  
  try {
    const callbackData = req.body;
    
    // Validate the callback data
    if (!callbackData.task_id || !callbackData.status) {
      return res.status(400).send('Invalid callback data');
    }
    
    // Get the job from the database
    const jobsService = new ItemsService('crawl_jobs', {
      schema: req.schema,
      accountability: req.accountability
    });
    
    const jobs = await jobsService.readByQuery({
      filter: { task_id: callbackData.task_id }
    });
    
    if (jobs.length === 0) {
      return res.status(404).send('Job not found');
    }
    
    const job = jobs[0];
    
    // Update the job status
    const updateData = {
      status: callbackData.status
    };
    
    if (callbackData.status === 'completed') {
      updateData.completed_at = new Date().toISOString();
    } else if (callbackData.status === 'failed' && callbackData.error) {
      updateData.error_message = callbackData.error;
    }
    
    await jobsService.updateOne(job.id, updateData);
    
    // If the job completed successfully, store the results
    if (callbackData.status === 'completed' && callbackData.result) {
      const resultsService = new ItemsService('crawl_results', {
        schema: req.schema,
        accountability: req.accountability
      });
      
      // Get the configuration to retrieve the URL
      const configsService = new ItemsService('crawl_configurations', {
        schema: req.schema,
        accountability: req.accountability
      });
      
      const config = await configsService.readOne(job.configuration);
      
      // Create a new result record
      await resultsService.createOne({
        job: job.id,
        url: config.url,
        content: callbackData.result.content,
        links: callbackData.result.links,
        images: callbackData.result.images,
        metadata: callbackData.result.metadata,
        pages_crawled: callbackData.result.pages_crawled,
        crawl_time: callbackData.result.crawl_time
      });
    }
    
    return res.status(200).send('Callback processed successfully');
  } catch (error) {
    console.error('Error processing callback:', error);
    return res.status(500).send('Error processing callback');
  }
};
</pre>
    </div>
    
    <div class="step">
        <div class="step-number">Step 4: Create a Flow to Trigger Crawls</div>
        <p>Now we'll create a flow that triggers a crawl when a new crawl_job is created:</p>
        <ol>
            <li>Go to <strong>Settings</strong> > <strong>Flows</strong></li>
            <li>Click <strong>Create Flow</strong></li>
            <li>Enter "Trigger Crawl" as the name</li>
            <li>Set the status to "Active"</li>
            <li>Set the trigger to "Event Hook"</li>
            <li>Configure the trigger:
                <ul>
                    <li>Scope: "Items"</li>
                    <li>Action: "Create"</li>
                    <li>Collection: "crawl_jobs"</li>
                </ul>
            </li>
            <li>Click <strong>Create</strong></li>
        </ol>
        
        <p>Now add an operation to the flow:</p>
        <ol>
            <li>Click <strong>Create Operation</strong></li>
            <li>Select "Run Script" as the type</li>
            <li>Enter "Start Crawl" as the name</li>
            <li>Enter the following code in the script editor:</li>
        </ol>
        
        <pre>
module.exports = async function(data, { services, exceptions }) {
  const { ItemsService } = services;
  const { ServiceUnavailableException } = exceptions;
  
  const jobId = data.key;
  const jobData = data.payload;
  
  try {
    // Get the configuration for this job
    const configsService = new ItemsService('crawl_configurations', {
      schema: req.schema,
      accountability: req.accountability
    });
    
    const config = await configsService.readOne(jobData.configuration);
    
    // Update job status to running
    const jobsService = new ItemsService('crawl_jobs', {
      schema: req.schema,
      accountability: req.accountability
    });
    
    await jobsService.updateOne(jobId, {
      status: 'running',
      started_at: new Date().toISOString()
    });
    
    // Prepare the request to the CrawlAI API
    const requestBody = {
      url: config.url,
      depth: config.depth || 1,
      max_pages: config.max_pages || 100,
      extract_images: config.extract_images || false,
      follow_external_links: config.follow_external_links || false,
      callback_url: `${process.env.PUBLIC_URL}/webhook/crawl-callback`
    };
    
    // Add excluded domains if present
    if (config.excluded_domains) {
      requestBody.excluded_domains = config.excluded_domains;
    }
    
    // Call the CrawlAI API
    const response = await fetch('http://your-server:5000/crawl', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody)
    });
    
    if (!response.ok) {
      throw new Error(`API responded with status: ${response.status}`);
    }
    
    const result = await response.json();
    
    // Update the job with the task_id from the API
    await jobsService.updateOne(jobId, {
      task_id: result.task_id
    });
    
    return {
      task_id: result.task_id
    };
  } catch (error) {
    // Update job status to failed
    const jobsService = new ItemsService('crawl_jobs', {
      schema: req.schema,
      accountability: req.accountability
    });
    
    await jobsService.updateOne(jobId, {
      status: 'failed',
      error_message: error.message
    });
    
    throw new ServiceUnavailableException(error.message);
  }
};
</pre>
        
        <div class="note">
            <strong>Note:</strong> Replace <code>http://your-server:5000</code> with your actual CrawlAI API endpoint.
        </div>
    </div>
    
    <h2 id="setup-appsmith">Setting Up Appsmith</h2>
    <div class="step">
        <div class="step-number">Step 5: Install and Configure Appsmith</div>
        
        <p>Let's add Appsmith to our Docker Compose setup:</p>
        <ol>
            <li>Update your <code>docker-compose.yml</code> file to include Appsmith:</li>
        </ol>
        <pre>
version: '3'
services:
  directus:
    image: directus/directus:latest
    ports:
      - 8055:8055
    environment:
      KEY: 'replace-with-random-key'
      SECRET: 'replace-with-random-secret'
      ADMIN_EMAIL: 'admin@example.com'
      ADMIN_PASSWORD: 'directus-password'
      DB_CLIENT: 'sqlite3'
      DB_FILENAME: '/directus/database/data.db'
      PUBLIC_URL: 'http://localhost:8055'
    volumes:
      - ./directus/database:/directus/database
      - ./directus/uploads:/directus/uploads
    restart: unless-stopped
    
  appsmith:
    image: appsmith/appsmith-ce:latest
    ports:
      - 8080:80
    volumes:
      - ./appsmith-data:/appsmith-stacks
    restart: unless-stopped</pre>
        
        <ol start="2">
            <li>Start the updated Docker Compose setup:</li>
        </ol>
        <div class="code-block">
            <code>docker-compose up -d</code>
        </div>
        
        <ol start="3">
            <li>Access Appsmith at <a href="http://localhost:8080" target="_blank">http://localhost:8080</a></li>
            <li>Sign up for a new account or use the default credentials</li>
            <li>Create a new application called "CrawlAI Dashboard"</li>
        </ol>
    </div>
    
    <h2 id="integration">Integration Steps</h2>
    <div class="step">
        <div class="step-number">Step 6: Connect Appsmith to Directus</div>
        
        <h3>Creating a REST API Data Source</h3>
        <ol>
            <li>In Appsmith, go to <strong>Data Sources</strong> in the left sidebar</li>
            <li>Click <strong>+ New Data Source</strong></li>
            <li>Select <strong>REST API</strong></li>
            <li>Configure the data source:
                <ul>
                    <li>Name: Directus API</li>
                    <li>URL: <code>http://localhost:8055</code> (or your Directus URL)</li>
                </ul>
            </li>
            <li>Click <strong>Save</strong></li>
        </ol>
        
        <h3>Setting Up Authentication</h3>
        <ol>
            <li>Create a new query to authenticate with Directus:</li>
            <li>Click <strong>+ New API</strong> to create a new query</li>
            <li>Configure the query:
                <ul>
                    <li>Name: loginToDirectus</li>
                    <li>URL: <code>{{data_source.url}}/auth/login</code></li>
                    <li>Method: POST</li>
                    <li>Headers: Add <code>Content-Type: application/json</code></li>
                    <li>Body:
<pre>
{
  "email": "admin@example.com",
  "password": "directus-password"
}
</pre>
                    </li>
                </ul>
            </li>
            <li>Click <strong>Run</strong> to test the query</li>
            <li>You should receive an access token in the response</li>
            <li>Save the query</li>
        </ol>
        
        <h3>Storing the Authentication Token</h3>
        <ol>
            <li>Create a JavaScript query to store the token:</li>
            <li>Click <strong>+ New JS Object</strong></li>
            <li>Name it "authUtils"</li>
            <li>Add the following code:</li>
        </ol>
        
        <pre>
export default {
  async initialize() {
    try {
      const response = await loginToDirectus.run();
      if (response && response.data && response.data.access_token) {
        storeValue('directusToken', response.data.access_token);
        return true;
      }
      return false;
    } catch (error) {
      console.error('Authentication failed:', error);
      return false;
    }
  },
  
  getAuthHeader() {
    return {
      'Authorization': `Bearer ${appsmith.store.directusToken}`
    };
  }
}
</pre>
        
        <ol start="5">
            <li>Call this function when the app loads by adding it to the Page Load event</li>
        </ol>
    </div>
    
    <div class="step">
        <div class="step-number">Step 7: Create Basic CRUD Queries</div>
        <p>Let's create the basic queries we'll need for our application:</p>
        
        <h3>Crawl Configurations Queries</h3>
        
        <h4>Get All Configurations</h4>
        <ol>
            <li>Create a new API query:</li>
            <li>Name: getConfigurations</li>
            <li>URL: <code>{{data_source.url}}/items/crawl_configurations</code></li>
            <li>Method: GET</li>
            <li>Headers: Add <code>Content-Type: application/json</code> and <code>Authorization: Bearer {{appsmith.store.directusToken}}</code></li>
        </ol>
        
        <h4>Create Configuration</h4>
        <ol>
            <li>Create a new API query:</li>
            <li>Name: createConfiguration</li>
            <li>URL: <code>{{data_source.url}}/items/crawl_configurations</code></li>
            <li>Method: POST</li>
            <li>Headers: Add <code>Content-Type: application/json</code> and <code>Authorization: Bearer {{appsmith.store.directusToken}}</code></li>
            <li>Body:
<pre>
{
  "name": "{{configName.text}}",
  "url": "{{configUrl.text}}",
  "depth": {{configDepth.value || 1}},
  "max_pages": {{configMaxPages.value || 100}},
  "extract_images": {{configExtractImages.value || false}},
  "follow_external_links": {{configFollowExternal.value || false}},
  "excluded_domains": {{configExcludedDomains.value || []}}
}
</pre>
            </li>
        </ol>
        
        <h3>Crawl Jobs Queries</h3>
        
        <h4>Get All Jobs</h4>
        <ol>
            <li>Create a new API query:</li>
            <li>Name: getJobs</li>
            <li>URL: <code>{{data_source.url}}/items/crawl_jobs?sort=-created_at</code></li>
            <li>Method: GET</li>
            <li>Headers: Add <code>Content-Type: application/json</code> and <code>Authorization: Bearer {{appsmith.store.directusToken}}</code></li>
        </ol>
        
        <h4>Create Job</h4>
        <ol>
            <li>Create a new API query:</li>
            <li>Name: createJob</li>
            <li>URL: <code>{{data_source.url}}/items/crawl_jobs</code></li>
            <li>Method: POST</li>
            <li>Headers: Add <code>Content-Type: application/json</code> and <code>Authorization: Bearer {{appsmith.store.directusToken}}</code></li>
            <li>Body:
<pre>
{
  "configuration": "{{selectedConfigId}}",
  "status": "pending"
}
</pre>
            </li>
        </ol>
        
        <h3>Crawl Results Queries</h3>
        
        <h4>Get Results for Job</h4>
        <ol>
            <li>Create a new API query:</li>
            <li>Name: getResultsForJob</li>
            <li>URL: <code>{{data_source.url}}/items/crawl_results?filter[job][_eq]={{selectedJobId}}</code></li>
            <li>Method: GET</li>
            <li>Headers: Add <code>Content-Type: application/json</code> and <code>Authorization: Bearer {{appsmith.store.directusToken}}</code></li>
        </ol>
    </div>
    
    <h2 id="building-ui">Building the User Interface</h2>
    <div class="step">
        <div class="step-number">Step 8: Create the Dashboard Layout</div>
        
        <h3>Setting Up the Main Dashboard</h3>
        <ol>
            <li>In your Appsmith application, go to the default page</li>
            <li>Add a Container widget to create a header</li>
            <li>Add a Text widget with the title "CrawlAI Dashboard"</li>
            <li>Add a Tabs widget with the following tabs:
                <ul>
                    <li>Configurations</li>
                    <li>Jobs</li>
                    <li>Results</li>
                    <li>Analytics</li>
                </ul>
            </li>
        </ol>
        
        <h3>Building the Configurations Tab</h3>
        <ol>
            <li>In the Configurations tab, add a Table widget to display configurations:
                <ul>
                    <li>Name: configurationsTable</li>
                    <li>Data: <code>{{getConfigurations.data.data}}</code></li>
                    <li>Columns: name, url, depth, max_pages, created_at</li>
                </ul>
            </li>
            <li>Add a Button widget to create new configurations:
                <ul>
                    <li>Label: "New Configuration"</li>
                    <li>onClick: Show a modal for creating configurations</li>
                </ul>
            </li>
            <li>Add a Button widget in each row to start a crawl:
                <ul>
                    <li>Label: "Start Crawl"</li>
                    <li>onClick: Run the createJob query with the selected configuration ID</li>
                </ul>
            </li>
        </ol>
        
        <h3>Creating the New Configuration Modal</h3>
        <ol>
            <li>Add a Modal widget:
                <ul>
                    <li>Name: newConfigModal</li>
                    <li>Title: "Create New Crawl Configuration"</li>
                </ul>
            </li>
            <li>Inside the modal, add form inputs:
                <ul>
                    <li>Input widget for Name</li>
                    <li>Input widget for URL</li>
                    <li>Number Input widget for Depth</li>
                    <li>Number Input widget for Max Pages</li>
                    <li>Switch widget for Extract Images</li>
                    <li>Switch widget for Follow External Links</li>
                    <li>Array Input widget for Excluded Domains</li>
                </ul>
            </li>
            <li>Add a Button widget to submit the form:
                <ul>
                    <li>Label: "Create"</li>
                    <li>onClick: Run the createConfiguration query and close the modal</li>
                </ul>
            </li>
        </ol>
        
        <h3>Building the Jobs Tab</h3>
        <ol>
            <li>In the Jobs tab, add a Table widget to display jobs:
                <ul>
                    <li>Name: jobsTable</li>
                    <li>Data: <code>{{getJobs.data.data}}</code></li>
                    <li>Columns: configuration.name, status, started_at, completed_at</li>
                </ul>
            </li>
            <li>Add a Button widget to refresh the jobs list:
                <ul>
                    <li>Label: "Refresh"</li>
                    <li>onClick: Run the getJobs query</li>
                </ul>
            </li>
            <li>Add a custom column for status with colored indicators:
<pre>
// Custom column for status
return (
  &lt;div&gt;
    &lt;span style={{
      display: 'inline-block',
      width: '10px',
      height: '10px',
      borderRadius: '50%',
      backgroundColor: 
        currentRow.status === 'completed' ? 'green' :
        currentRow.status === 'running' ? 'blue' :
        currentRow.status === 'pending' ? 'orange' : 'red',
      marginRight: '5px'
    }}&gt;&lt;/span&gt;
    {currentRow.status}
  &lt;/div&gt;
);
</pre>
            </li>
        </ol>
        
        <h3>Building the Results Tab</h3>
        <ol>
            <li>In the Results tab, add a Dropdown widget to select a job:
                <ul>
                    <li>Name: jobSelector</li>
                    <li>Options: <code>{{getJobs.data.data.map(job => ({ label: job.configuration.name + ' (' + job.created_at + ')', value: job.id }))}}</code></li>
                    <li>onOptionChange: Set selectedJobId and run getResultsForJob</li>
                </ul>
            </li>
            <li>Add a Container widget to display result details:
                <ul>
                    <li>Visible: <code>{{getResultsForJob.data.data.length > 0}}</code></li>
                </ul>
            </li>
            <li>Inside the container, add:
                <ul>
                    <li>Text widget for URL: <code>{{getResultsForJob.data.data[0].url}}</code></li>
                    <li>Text widget for Pages Crawled: <code>{{getResultsForJob.data.data[0].pages_crawled}}</code></li>
                    <li>Text widget for Crawl Time: <code>{{getResultsForJob.data.data[0].crawl_time}} seconds</code></li>
                </ul>
            </li>
            <li>Add a Tabs widget for different result views:
                <ul>
                    <li>Content (Markdown viewer)</li>
                    <li>Links (Table of links)</li>
                    <li>Images (Grid of images)</li>
                </ul>
            </li>
        </ol>
        
        <h3>Building the Analytics Tab</h3>
        <ol>
            <li>In the Analytics tab, add charts to visualize crawl data:
                <ul>
                    <li>Chart widget for Crawl Times</li>
                    <li>Chart widget for Pages Crawled</li>
                    <li>Chart widget for Status Distribution</li>
                </ul>
            </li>
            <li>Add a query to get analytics data:
                <ul>
                    <li>Name: getAnalytics</li>
                    <li>URL: <code>{{data_source.url}}/items/crawl_results?aggregate[avg]=crawl_time&aggregate[sum]=pages_crawled&aggregate[count]=*</code></li>
                </ul>
            </li>
            <li>Use JavaScript to transform the data for charts:
<pre>
// Transform data for charts
const jobsData = getJobs.data.data;
const resultsData = getResultsForJob.data.data;

// Status distribution
const statusCounts = {
  pending: 0,
  running: 0,
  completed: 0,
  failed: 0
};

jobsData.forEach(job => {
  statusCounts[job.status] = (statusCounts[job.status] || 0) + 1;
});

const statusChartData = Object.keys(statusCounts).map(status => ({
  x: status,
  y: statusCounts[status]
}));

return statusChartData;
</pre>
            </li>
        </ol>
    </div>
    
    <h2 id="advanced">Advanced Features</h2>
    <div class="step">
        <div class="step-number">Step 9: Implementing Scheduled Crawls</div>
        
        <h3>Creating a Scheduler in Directus</h3>
        <ol>
            <li>Go to <strong>Settings</strong> > <strong>Flows</strong></li>
            <li>Create a new flow with a Schedule trigger</li>
            <li>Set it to run every hour (or your preferred schedule)</li>
            <li>Add a script operation to check for scheduled crawls:</li>
        </ol>
        
        <pre>
module.exports = async function(data, { services, exceptions }) {
  const { ItemsService } = services;
  
  try {
    // Get configurations with schedules
    const configsService = new ItemsService('crawl_configurations', {
      schema: req.schema,
      accountability: req.accountability
    });
    
    const configs = await configsService.readByQuery({
      filter: {
        schedule: {
          _nnull: true
        }
      }
    });
    
    // Check which ones should run now
    const now = new Date();
    const jobsToCreate = [];
    
    for (const config of configs) {
      if (!config.schedule) continue;
      
      // Simple schedule check (in a real app, use a cron parser)
      const shouldRun = checkSchedule(config.schedule, now);
      
      if (shouldRun) {
        jobsToCreate.push(config.id);
      }
    }
    
    // Create jobs for scheduled crawls
    const jobsService = new ItemsService('crawl_jobs', {
      schema: req.schema,
      accountability: req.accountability
    });
    
    for (const configId of jobsToCreate) {
      await jobsService.createOne({
        configuration: configId,
        status: 'pending'
      });
    }
    
    return {
      message: `Created ${jobsToCreate.length} scheduled jobs`
    };
  } catch (error) {
    console.error('Scheduler error:', error);
    return {
      error: error.message
    };
  }
};

// Simple schedule checker (replace with proper cron parser in production)
function checkSchedule(schedule, now) {
  // For demo purposes, assume schedule is a simple hour value (0-23)
  const hour = parseInt(schedule);
  return !isNaN(hour) && hour === now.getHours();
}
</pre>
        
        <h3>Adding Schedule UI in Appsmith</h3>
        <ol>
            <li>Update the configuration form to include a schedule field</li>
            <li>Add a Dropdown widget for common schedules:
                <ul>
                    <li>Options: Hourly, Daily, Weekly, Monthly</li>
                </ul>
            </li>
            <li>Add a custom input for cron expressions</li>
        </ol>
    </div>
    
    <div class="step">
        <div class="step-number">Step 10: Adding User Management</div>
        
        <h3>Setting Up User Roles in Directus</h3>
        <ol>
            <li>In Directus, go to <strong>Settings</strong> > <strong>Roles & Permissions</strong></li>
            <li>Create roles for different user types:
                <ul>
                    <li>Admin: Full access</li>
                    <li>Editor: Can create and run crawls</li>
                    <li>Viewer: Can only view results</li>
                </ul>
            </li>
            <li>Configure permissions for each role</li>
        </ol>
        
        <h3>Creating a Login Page in Appsmith</h3>
        <ol>
            <li>Create a new page for login</li>
            <li>Add form inputs for email and password</li>
            <li>Add a login button that calls the Directus authentication API</li>
            <li>Store the user role in Appsmith storage</li>
            <li>Implement conditional rendering based on user role</li>
        </ol>
        
        <h3>Implementing Role-Based Access Control</h3>
        <ol>
            <li>Use JavaScript to show/hide UI elements based on user role:
<pre>
// Check if user has admin privileges
const isAdmin = appsmith.store.userRole === 'admin';

// Return true/false to control widget visibility
return isAdmin;
</pre>
            </li>
            <li>Apply this logic to buttons and other interactive elements</li>
        </ol>
    </div>
    
    <div class="step">
        <div class="step-number">Step 11: Implementing Content Analysis</div>
        
        <h3>Adding Text Analysis Features</h3>
        <ol>
            <li>Create a new collection in Directus for analysis results</li>
            <li>Add a flow in Directus that processes crawl results to extract:
                <ul>
                    <li>Keywords</li>
                    <li>Entities (people, places, organizations)</li>
                    <li>Sentiment analysis</li>
                </ul>
            </li>
            <li>Create a visualization in Appsmith to display analysis results</li>
        </ol>
        
        <h3>Implementing Comparison Features</h3>
        <ol>
            <li>Add a feature to compare results from different crawls</li>
            <li>Create a diff viewer to highlight changes between crawls</li>
            <li>Add charts to visualize changes over time</li>
        </ol>
    </div>
    
    <h2>Conclusion</h2>
    <p>
        Congratulations! You've successfully built a complete web crawling system by integrating:
    </p>
    <ul>
        <li><strong>CrawlAI API</strong> for powerful web crawling capabilities</li>
        <li><strong>Directus</strong> for flexible data storage and management</li>
        <li><strong>Appsmith</strong> for creating a user-friendly dashboard</li>
    </ul>
    
    <p>
        This system allows you to:
    </p>
    <ul>
        <li>Configure and trigger web crawls</li>
        <li>Store and manage crawled data</li>
        <li>Visualize and analyze crawl results</li>
        <li>Schedule automated crawls</li>
        <li>Implement user roles and permissions</li>
    </ul>
    
    <p>
        You can extend this system further with:
    </p>
    <ul>
        <li>Advanced content analysis using NLP</li>
        <li>Integration with other data processing tools</li>
        <li>Custom reporting and export features</li>
        <li>Mobile-responsive interfaces</li>
        <li>Multi-language support</li>
    </ul>
    
    <div class="note">
        <strong>Note:</strong> Remember to secure your API endpoints and implement proper authentication
        between all components in a production environment.
    </div>
    
    <footer>
        <p>CrawlAI + Directus + Appsmith Integration Tutorial &copy; 2025</p>
    </footer>
</body>
</html>
